{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization: introduction\n",
    "\n",
    "* Optimization or mathematical programming considers the problem\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\text{minimize} & f(\\mathbf{x}) \\\\\n",
    "    \\text{subject to} & \\mathbf{x} \\in C\n",
    "\\end{array}\n",
    "$$\n",
    "    - Vector $\\mathbf{x}$  in a vector space $V$ is the *optimization variable*.\n",
    "        + In most cases $V=\\mathbb{R}^d$.\n",
    "    - Function $f: V \\to \\mathbb{R}$ is the *objective function*.\n",
    "    - $C \\subset V$ is called the *constraint set*.\n",
    "        + If $C = V$, the problem is *unconstrained*.\n",
    "        + Otherwise it is constrained.\n",
    "    - $\\mathbf{x}^{\\star}$ is called a (global) *solution* if $f(\\mathbf{x}^{\\star}) \\le f(\\mathbf{x})$ for all $\\mathbf{x}\\in C$.\n",
    "        + Solution may not be unique (recall the least squares problem).\n",
    "        + Solution may not even exist (e.g., $f(x)=1/x$, $C=\\{x\\in\\mathbb{R}: x \\ge 0\\}$).\n",
    "\n",
    "* Possible confusion:\n",
    "     * Statisticians mostly talk about **maximization**: $\\max \\, L(\\mathbf{\\theta})$.\n",
    "     * Optimizers talk about **minimization**: $\\min \\, f(\\mathbf{x})$.\n",
    "\n",
    "* **Why** is optimization important in statistics? \n",
    "    * Maximum likelihood estimation (MLE). \n",
    "    * Maximum a posteriori (MAP) estimation in Bayesian framework.  \n",
    "    * Machine learning: minimize a loss + certain regularization.  \n",
    "    * ...\n",
    "    \n",
    "* Global optimization\n",
    "    * Worst-case complexity grows exponentially with the $d$ and $n$\n",
    "    * Even small problems, with a few tens of variables, can take a very long time (e.g., hours or days) to solve.\n",
    "    \n",
    "* Local optimization\n",
    "    * *Local* solution: $\\mathbf{x}^{\\dagger}$ such that $f(\\mathbf{x}^{\\dagger}) \\le f(\\mathbf{x})$ for all $\\mathbf{x}\\in C \\cap \\mathbf{N}(\\mathbf{x}^{\\dagger})$, where $\\mathbf{N}(\\mathbf{x}^{\\dagger})$ is a certain neighborhood of $\\mathbf{x}^{\\dagger}$.\n",
    "    * Can be found relatively easy, using only local information on $f$, e.g., gradients.\n",
    "    * Local optimization methods can be fast, can handle large-scale problems, and are widely applicable.\n",
    "    \n",
    "* Our major **goal** (or learning objectives) is to\n",
    "    * have a working knowledge of some commonly used optimization methods: \n",
    "        * convex programming with emphasis in statistical applications    \n",
    "        * Newton-type algorithms\n",
    "        * first-order methods\n",
    "        * expectation-maximization (EM) algorithm \n",
    "        * majorization-minimization (MM) algorithm  \n",
    "    * implement some of them in homework\n",
    "    * get to know some optimization tools in Julia\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Optimization 101\n",
    "\n",
    "### Convex optimization\n",
    "\n",
    "<img src=\"http://stanford.edu/~boyd/cvxbook/bv_cvxbook_cover.jpg\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "\n",
    "<img src=\"http://www.mit.edu/~dimitrib/convexdualitycover.jpg\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "\n",
    "* Problem\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\text{minimize} & f(\\mathbf{x}) \\\\\n",
    "    \\text{subject to} & \\mathbf{x} \\in C\n",
    "\\end{array}\n",
    "$$\n",
    "    - $f$ is a *convex function*\n",
    "$$\n",
    "        f(\\alpha\\mathbf{x} + (1-\\alpha)\\mathbf{y}) \\le \\alpha f(\\mathbf{x}) + (1-\\alpha) f(\\mathbf{y}), \\quad \\forall\\mathbf{x}, \\mathbf{y} \\in \\text{dom}f, \\forall\\alpha \\in [0, 1].\n",
    "$$\n",
    "    - $C$ is a *convex set*\n",
    "$$\n",
    "        \\mathbf{x}, \\mathbf{y} \\in C \\implies \\alpha\\mathbf{x} + (1-\\alpha)\\mathbf{y} \\in C, \\quad \\forall\\alpha \\in [0, 1].\n",
    "$$\n",
    "    \n",
    "* More familiar formulation: take $V=\\mathbb{R}^d$,\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "    \\text{minimize} & f_0(\\mathbf{x}) \\\\\n",
    "    \\text{subject to} & f_i(\\mathbf{x}) \\le b_i, \\quad i=1, 2, \\dotsc, m.\n",
    "\\end{array}\n",
    "$$\n",
    "where $f_i: \\mathbb{R}^d \\to \\mathbb{R}$ are convex functions.\n",
    "    - Equality constraint: $f_i(\\mathbf{x})=b_i \\iff f_i(\\mathbf{x}) \\le b_i$ and $-f_i(\\mathbf{x}) \\le -b_i$\n",
    "        + Hence only *linear* equality constraints $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ are allowed (why?)\n",
    "        \n",
    "* Why do we care about convex optimization?\n",
    "> **Fact**. Any local solution of a convex optimization problem is a global solution.\n",
    "\n",
    "* Role of convex optimization\n",
    "    - Initialization for local optimization\n",
    "    - Convex heuristics for nonconvex optimization\n",
    "    - Bounds for global optimization\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex sets\n",
    "\n",
    "The underlying space is a vector space $V$ unless otherwise stated.\n",
    "\n",
    "* Line segments: for given $\\mathbf{x}$, $\\mathbf{y}$,\n",
    "$$\n",
    "    \\{\\alpha \\mathbf{x} + (1-\\alpha)\\mathbf{y}: 0 \\le \\alpha \\le 1\\}.\n",
    "$$\n",
    "\n",
    "* A set $C$ is convex if for every pair of points $\\mathbf{x}$ and $\\mathbf{y}$ lying in $C$ the entire line segment connecting them also lies in $C$.\n",
    "\n",
    "<img src=\"https://www.easycalculation.com/maths-dictionary/images/convex-nonconvex-set.png\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examples\n",
    "    1. Singleton: $\\{\\mathbf{a}\\}$.\n",
    "    1. Euclidean space $\\mathbb{R}^d$.\n",
    "    1. Norm ball: $B_r(\\mathbf{c})=\\{\\mathbf{x}: \\|\\mathbf{x}-\\mathbf{c}\\| \\le r\\}$ for any proper norm $\\|\\cdot\\|$.\n",
    "    \n",
    "      <img src=\"https://i.stack.imgur.com/4KSgs.png\" width=\"500\" align=\"center\"/>\n",
    "\n",
    "      $\\ell_p$ \"norm\" $\\|\\mathbf{x}\\|_p = (\\sum_{i=1}^d |x_i|^p)^{1/p}$, $0<p<1$, in $\\mathbb{R}^d$ is *not* a proper norm.\n",
    "      \n",
    "    1. Hyperplane: $\\{\\mathbf{x}: \\langle \\mathbf{a}, \\mathbf{x} \\rangle = c \\}$.\n",
    "    1. Halfspace: $\\{\\mathbf{x}: \\langle \\mathbf{a}, \\mathbf{x} \\rangle \\le c \\}$ or $\\{\\mathbf{x}: \\langle \\mathbf{a}, \\mathbf{x} \\rangle < c \\}$.\n",
    "    1. Polyhedron: $\\{\\mathbf{x}: \\langle \\mathbf{a}_j, \\mathbf{x} \\rangle \\le b_j,~j=1,\\dots,m\\} = \\{\\mathbf{x}: \\mathbf{A}\\mathbf{x} \\le \\mathbf{b}\\}$.\n",
    "    1. Positive semidefinite cone $\\mathbb{S}^d_{+}=\\{\\mathbf{X}\\in\\mathbb{R}^{d\\times d}: \\mathbf{X} = \\mathbf{X}^T, ~\\mathbf{X} \\succeq \\mathbf{0} \\}$ and set of positive definite matrices $\\mathbb{S}^d_{++}=\\{\\mathbf{X}\\in\\mathbb{R}^{d\\times d}: \\mathbf{X}=\\mathbf{X}^T, ~\\mathbf{X} \\succ \\mathbf{0} \\}$. (What is $V$?)\n",
    "    1. Translation: $C + \\mathbf{a} = \\{\\mathbf{x} + \\mathbf{a}: \\mathbf{x}\\in C\\}$ if $C$ is convex.\n",
    "    1. Minkowski sum: $C + D=\\{\\mathbf{x}+\\mathbf{y}: \\mathbf{x}\\in C, \\mathbf{y}\\in D\\}$ if $C$ and $D$ are convex.   \n",
    "    1. Cartesian product: $C\\times D=\\{(\\mathbf{x},\\mathbf{y}): \\mathbf{x}\\in C, \\mathbf{y}\\in D\\}$ if $C$ and $D$ are convex. \n",
    "    1. Image $\\mathcal{A}C$ of a convex set $C$ under a linear map $\\mathcal{A}$.\n",
    "    1. Inverse image $\\mathcal{A}^{-1}C$ of a convex set $C$ under a linear map $\\mathcal{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex cones\n",
    "\n",
    "* A set $C$ is a cone if for each $\\mathbf{x}$ in $C$, the set $\\{\\alpha\\mathbf{x}: \\alpha > 0\\} \\subset C$.\n",
    "    + Some authors (e.g., Boyd & Vandenberghe) use instead $\\{\\alpha\\mathbf{x}: \\alpha \\ge 0\\}$ to include the origin.\n",
    "    + Is $\\mathbb{S}_{++}^d$ a cone?\n",
    "    + A cone is *unbounded*.\n",
    "\n",
    "* A cone $C$ is a convex cone if it is also convex. Equivalently, a set $C$ is a convex cone if for any $\\mathbf{x}, \\mathbf{y}\\in C$,  $\\{\\alpha\\mathbf{x} + \\beta\\mathbf{y}: \\alpha, \\beta > 0\\} \\subset C$.\n",
    "\n",
    "* Examples of convex cone\n",
    "    1. Any subspace\n",
    "    1. Any hyperplane passing the origin.\n",
    "    1. Any halfspace whose closure passes the origin.\n",
    "    1. $\\mathbb{S}_{+}^d$, set of positive semidefinite matrices.\n",
    "    1. Second-order cone (Lorentz cone): $\\{(\\mathbf{x}, t): \\|\\mathbf{x}\\|_2 \\le t \\}$.\n",
    "    \n",
    "      <img src=\"https://www.researchgate.net/profile/Tapio_Saramaki/publication/3450705/figure/fig1/AS:340710273372163@1458243067573/The-second-order-cone-in-R-3.png\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "    1. Norm cone: $\\{(\\mathbf{x}, t): \\|\\mathbf{x}\\| \\le t \\}$, where $\\|\\cdot\\|$ is any norm.\n",
    "    \n",
    "* Example of a nonconvex cone?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine sets\n",
    "\n",
    "* A set $C$ is a affine if for each $\\mathbf{x}, \\mathbf{y}$ in $C$, the whole line $\\{\\alpha\\mathbf{x}+(1-\\alpha)\\mathbf{y}: \\alpha \\text{ is a scalar} \\} \\subset C$.\n",
    "* Convex.\n",
    "* $C = \\mathbf{c} + S$ for some $\\mathbf{c}\\in V$ and subspace $S$.\n",
    "    - We define $\\text{dim}C = \\text{dim}S$.\n",
    "* Example\n",
    "    - Any singleton.\n",
    "    - Any subspace.\n",
    "    - Set of solutions of a linear system of equations: $\\{\\mathbf{x}: \\mathbf{A}\\mathbf{x}=\\mathbf{b}\\}$.\n",
    "        + In fact *any* affine set can be represented by the solution set of a linear system.\n",
    "        \n",
    "**The intersection of an arbitrary collection of convex, affine, or conical sets is convex, affine, conical, respectively.**        \n",
    "        \n",
    "### Generators\n",
    "\n",
    "* Convex combination: $\\{\\sum_{i=1}^m \\alpha_i \\mathbf{x}_i: \\alpha_i \\ge 0, ~\\sum_{i=1}^m\\alpha_i = 1\\}$\n",
    "\n",
    "* Convex sets are closed under convex combination: any convex combination of points from a convex set $C$ belongs to $C$\n",
    "\n",
    "* Conic combination, affine conbination are defined similarly; similar closure properties also hold.\n",
    "    \n",
    "* **Convex hull**: the convex hull of a nonempty set $C$ is the set of all convex combinations of points in $C$:    \n",
    "$$\n",
    "    \\text{conv}C = \\{\\sum_{i=1}^m \\alpha_i \\mathbf{x}_i: \\mathbf{x}_i\\in C,~ \\alpha_i \\ge 0, i=1,\\dotsc,m \\text{ for some }m,~\\sum_{i=1}^m\\alpha_i = 1 \\}\n",
    "$$\n",
    "    which is the *smallest convex set containing* $C$.\n",
    "    \n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/ConvexHull.svg/220px-ConvexHull.svg.png\" width=\"200\" align=\"center\"/>\n",
    "\n",
    "    \n",
    "* **Conic hull** $\\text{cone}C$ drops the sum condition, and **affine hull** $\\text{aff}C$ drops the nonnegativity conditions on $\\alpha_i$s.\n",
    "\n",
    "    <img src=\"https://holovincent.files.wordpress.com/2012/11/conic-hull.png\" width=\"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Affine dimension**: $\\text{dim}(C) \\triangleq \\text{dim}(\\text{aff}C)$.\n",
    "\n",
    "* Example: simplex\n",
    "$$\n",
    "S = \\text{conv}\\{\\mathbf{v}_0, \\mathbf{v}_1, \\dotsc, \\mathbf{v}_k\\}\n",
    "$$\n",
    "when $\\mathbf{v}_0, \\mathbf{v}_1, \\dotsc, \\mathbf{v}_k$ are *affinely independent*, i.e., $\\mathbf{v}_1-\\mathbf{v}_0, \\dotsc, \\mathbf{v}_k-\\mathbf{v}_0$ are linearly independent.\n",
    "    - $\\dim(S) = k$\n",
    "    - Unit simplex in $\\mathbb{R}^d$: $\\text{conv}\\{\\mathbf{0}, \\mathbf{e}_1, \\dots, \\mathbf{e}_d\\} = \\{\\mathbf{x}\\in\\mathbb{R}^d: \\mathbf{x} \\ge \\mathbf{0}, ~ \\mathbf{1}^T\\mathbf{x} \\le 1 \\}$.\n",
    "    - Probability simplex in $\\mathbb{R}^d$: $\\Delta_{d-1} = \\text{conv}\\{\\mathbf{e}_1, \\dots, \\mathbf{e}_d\\} = \\{\\mathbf{x}\\in\\mathbb{R}^d: \\mathbf{x} \\ge \\mathbf{0}, ~ \\mathbf{1}^T\\mathbf{x} = 1 \\}$.\n",
    "\n",
    "### Relative interior\n",
    "\n",
    "Most constraint sets in optimization does not have an interior, e.g, probability simplex. It is useful to define the interior relative to the affine hull:\n",
    "$$\n",
    "    \\text{relint}C = \\{\\mathbf{x}\\in C: (\\exists r>0) B(\\mathbf{x}, r) \\cap \\text{aff}C \\subset C \\}\n",
    "$$\n",
    "\n",
    "* What is the relative interior of the probability simplex in $\\mathbb{R}^3$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex functions\n",
    "\n",
    "* Recall that a real-valued function $f$ is convex if\n",
    "$$\n",
    "    f(\\alpha\\mathbf{x} + (1-\\alpha)\\mathbf{y}) \\le \\alpha f(\\mathbf{x}) + (1-\\alpha) f(\\mathbf{y}), \\quad \\forall\\mathbf{x}, \\mathbf{y} \\in \\text{dom}f, \\forall\\alpha \\in [0, 1],\n",
    "$$\n",
    "\n",
    "    - If the inequality is strict for all $\\alpha \\in (0,1)$, then $f$ is *strictly convex*.\n",
    "    - $f$ is concave if $-f$ is convex.\n",
    "\n",
    "* Extended-value functions: it is often convenient to extend the domain of $f$ to the whole $V$ and allow to have the value $\\infty$. Then $f:V \\to \\mathbb{R}\\cup\\{\\infty\\}$ and\n",
    "$$\n",
    "    \\text{dom} f = \\{\\mathbf{x}: f(\\mathbf{x}) < \\infty \\}\n",
    "$$\n",
    "is the **essential domain** of $f$.\n",
    "\n",
    "* This extension allows us to consider the *indicator function* of a set:\n",
    "$$\n",
    "    \\iota_C(\\mathbf{x}) = \\begin{cases} 0 & \\mathbf{x} \\in C \\\\ \\infty & \\mathbf{x} \\notin C \\end{cases}\n",
    "$$\n",
    "so that a constrained optimization problem is converted to an unconstrained problem:\n",
    "$$\n",
    "    \\min_{\\mathbf{x}\\in C} f(\\mathbf{x}) = \\min_{\\mathbf{x}} f(\\mathbf{x}) + \\iota_C(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "* Properness: a function $f$ is proper if $\\text{dom}f \\neq \\emptyset$ and $f(\\mathbf{x}) > -\\infty$ for all $\\mathbf{x}$.\n",
    "\n",
    "* Examples\n",
    "    1. Any affine function (also concave).\n",
    "    1. Any norm\n",
    "    1. Indicator function of a nonempty convex set.\n",
    "    1. Exponential: $f(x)=e^{ax}$.\n",
    "    1. Powers: $f(x)=x^\\alpha$ on $\\mathbb{R}_{++}=\\{x\\in\\mathbb{R}: x>0\\}$. Convex if $\\alpha\\ge 1$, concave if $\\alpha\\in[0,1]$.\n",
    "    1. Powers of absolute values: $f(x)=|x|^p$ on $\\mathbb{R}$, if $p \\ge 1$.\n",
    "    1. Logarithm: $f(x)=\\log x$ is *concave* in $\\text{dom}f = \\mathbb{R}_{++}$.\n",
    "    1. Quadratic-over-linear function $f(x,y) = x^2/y$ with $\\text{dom}f = \\mathbb{R}\\times\\mathbb{R}_{++} = \\{(x,y): x\\in\\mathbb{R}, y > 0\\}$.\n",
    "    1. Maximum: $f(\\mathbf{x}) = \\max\\{x_1,\\dotsc,x_d\\}$.\n",
    "    1. Log-sum-exp: $f(\\mathbf{x}) = \\log(e^{x_1}+\\dotsb+e^{x_d})$, a smoothed version of $\\max\\{x_1,\\dotsc,x_d\\}$.\n",
    "    1. Geometric mean: $f(\\mathbf{x}) = \\prod_{i=1}^d x_i^{1/d}$ is *concave*, with $\\text{dom}f=\\mathbb{R}_{++}^d$.\n",
    "    1. Log-determinent: $f(\\mathbf{X}) = \\log\\det\\mathbf{X}$ is *concave* in $\\text{dom}f = \\mathbb{S}_{++}^d$.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jensen's inequality\n",
    "\n",
    "* Funciton $f$ is convex **if and only if**\n",
    "$$\n",
    "    f(\\sum_{i=1}^m\\mathbf{x}_i) \\le \\sum_{i=1}^m\\alpha_i f(\\mathbf{x}_i), \\quad  \\forall \\mathbf{x}_1, \\dotsc, \\mathbf{x}_m, ~\\forall\\alpha_i \\ge 0, ~ \\sum_{i=1}^m \\alpha_i=1.\n",
    "$$\n",
    "\n",
    "\n",
    "### First-order condition (supporting hyperplane inequality)\n",
    "\n",
    "* If $f$ is differentiable (i.e., its gradient $\\nabla f$ exists at each point in $\\text{dom}f$, which is open), then $f$ is convex **if and only if** $\\text{dom} f$ is convex and\n",
    "$$\n",
    "    f(\\mathbf{y}) \\ge f(\\mathbf{x}) + \\langle \\nabla f(\\mathbf{x}), \\mathbf{y}-\\mathbf{x} \\rangle\n",
    "$$\n",
    "for all $\\mathbf{x}, \\mathbf{y} \\in \\text{dom}f$.\n",
    "    - $f$ is strictly convex if and only if strict inequality holds for all $\\mathbf{y} \\neq \\mathbf{x}$.\n",
    "    \n",
    "    <img src=\"https://www.researchgate.net/publication/335250822/figure/download/fig38/AS:793778201821204@1566262879662/First-order-Condition-for-Convex-Function.ppm\" width=\"400\" align=\"center\"/>    \n",
    "    \n",
    "    \n",
    "### Second-order condition\n",
    "\n",
    "* If $f$ is twice differentiable (i.e., its Hessian $\\nabla^2 f$ exists at each point in $\\text{dom}f$, which is open), then $f$ is convex **if and only if** $\\text{dom} f$ is convex and its Hessian is positive semidefinite:, i.e, \n",
    "$$\n",
    "    \\nabla^2 f(\\mathbf{x}) \\succeq \\mathbf{0}\n",
    "$$\n",
    "for all $\\mathbf{x} \\in \\text{dom} f$.\n",
    "    - If $\\nabla^2 f(\\mathbf{x}) \\succ \\mathbf{0}$, then $f$ is strictly convex.\n",
    "    \n",
    "    \n",
    "### Epigraph\n",
    "\n",
    "* The epigraph of a function $f$ is the set\n",
    "$$\n",
    "    \\text{epi}f = \\{(\\mathbf{x}, t): \\mathbf{x}\\in\\text{dom}f,~ f(\\mathbf{x}) \\le t \\}.\n",
    "$$\n",
    "\n",
    "* A function $f$ is convex **if and only if** $\\text{epi}f$ is convex.\n",
    "\n",
    "    <img src=\"./epigraph.png\" width=\"330\" align=\"center\"/>    \n",
    "\n",
    "* If $(\\mathbf{y}, t)\\in\\text{epi}f$, then from the supporting hyperplance inequality, \n",
    "$$\n",
    "    t \\ge f(\\mathbf{y}) \\ge f(\\mathbf{x}) + \\langle \\nabla f(\\mathbf{x}), \\mathbf{y} - \\mathbf{x} \\rangle\n",
    "$$\n",
    "or\n",
    "$$\n",
    "    \\left\\langle (\\nabla f(\\mathbf{x}), -1), (\\mathbf{y}, t) - (\\mathbf{x}, f(\\mathbf{x})) \\right\\rangle \\le 0.\n",
    "$$\n",
    "This means that the hyperplane defined by $(\\nabla f(\\mathbf{x}),−1)$ supports $\\text{epi}f$ at the\n",
    "boundary point $(\\mathbf{x},f(\\mathbf{x}))$:\n",
    "\n",
    "    <img src=\"https://www.researchgate.net/profile/Beatrice_Pesquet/publication/270905703/figure/fig1/AS:614208265793537@1523450070242/Projection-onto-the-epigraph-of-a-function-ph-G-0-R.png\" width=\"300\" align=\"center\"/>    \n",
    "\n",
    "* An extended-value function $f$ is called *closed* if $\\text{epi}f$ is closed.\n",
    "\n",
    "\n",
    "### Sublevel sets\n",
    "\n",
    "* $\\alpha$-sublevel set of an extended-value function $f$ is \n",
    "$$\n",
    "    S_{\\alpha} = \\{\\mathbf{x}\\in\\text{dom}f : f(\\mathbf{x}) \\le \\alpha \\}.\n",
    "$$\n",
    "\n",
    "* **If** $f$ is convex, then $S_{\\alpha}$ is convex for all $\\alpha$.\n",
    "    - Converse is not true: $f(x)=-e^{x}$.\n",
    "\n",
    "* Further if $f$ is continuous, then all sublevel sets are closed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations that preserve convexity\n",
    "\n",
    "1. (Nonnegative weighted sums) If $f$ and $g$ are convex and $\\alpha$ and $\\beta$ are nonnegative constants, then $\\alpha f + \\beta g$ is convex, with $\\text{dom}f \\cap \\text{dom}g$.\n",
    "    - Extension: If $f(\\mathbf{x},\\mathbf{y})$ is convex in $\\mathbf{x}$ for each fixed $\\mathbf{y} \\in \\mathcal{A}$, and $w(\\mathbf{y})\\ge 0$ for all $y\\in\\mathcal{A}$, then the integral $g(\\mathbf{x}) = 􏰠 \\int_{\\mathcal{A}} w(\\mathbf{y})f(\\mathbf{x},\\mathbf{y})d\\mathbf{y}$ is convex provided the integral exists.\n",
    "\n",
    "1. (Composition with affine mapping) If $f$ is convex, then  composition $f(\\mathbf{A}\\mathbf{x} + \\mathbf{b})$ of $f$  with an affine map $\\mathbf{x}\\mapsto \\mathbf{A}\\mathbf{x} + \\mathbf{b}$ is convex, with $\\text{dom}g=\\{\\mathbf{x}:\\mathbf{Ax}+\\mathbf{b}\\in\\text{dom}f\\}$.\n",
    "\n",
    "1. (Pointwise maximum and supremum) If $f_i$ is convex for each fixed $i=1,\\dotsc,m$, then $g(\\mathbf{x}) = \\max_{i=1,\\dotsc,m} f_i(\\mathbf{x})$ is convex, with $\\text{dom}g = \\cap_{i=1}^m\\text{dom}f_i$. \n",
    "    - Extension: If $f(\\mathbf{x},\\mathbf{y})$ is convex in $x$, then $g(\\mathbf{x}) = \\sup_{\\mathbf{y}\\in\\mathcal{A}}f(\\mathbf{x},\\mathbf{y})$ is convex,\n",
    "with $\\text{dom}g=\\{\\mathbf{x}:(\\forall\\mathbf{y}\\in\\mathcal{A})(\\mathbf{x},\\mathbf{y})\\in\\text{dom}f, \\sup_{\\mathbf{y}\\in\\mathcal{A}} f(\\mathbf{x},\\mathbf{y})<\\infty\\}$. \n",
    "\n",
    "1. (Composition) For $h:\\mathbb{R}\\to\\mathbb{R}\\cup\\{\\infty\\}$ and $g:V\\to\\mathbb{R}\\cup\\{\\infty\\}$, if $h$ is convex and *nondecreasing*, and $g$ is convex, then $f = h \\circ g$ is convex, with $\\text{dom}f = \\{\\mathbf{x}\\in\\text{dom}g: g(\\mathbf{x})\\in\\text{dom}h\\}$.\n",
    "\n",
    "1. (Paritial minimization) If $f(\\mathbf{x},\\mathbf{y})$ is **jointly** convex in $(\\mathbf{x},\\mathbf{y})$, then $g(\\mathbf{x}) = \\inf_{\\mathbf{y}\\in C} f(\\mathbf{x},\\mathbf{y})$ is convex provided it is proper and $C$ is nonempty convex, with $\\text{dom}f = \\{\\mathbf{x}: (\\exists\\mathbf{y}\\in C)(\\mathbf{x},\\mathbf{y})\\in\\text{dom}f\\}$ (projection).\n",
    "\n",
    "1. (Perspective) If $f(\\mathbf{x})$ is convex and *finite*, then its *perspective* $g(\\mathbf{x}, t)=t f(t^{-1}\\mathbf{x})$ is convex, with $\\text{dom}g=\\{(\\mathbf{x},t): t^{-1}\\mathbf{x}\\in\\text{dom}f,~ t>0\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "* A sum of convex functions is convex.\n",
    "\n",
    "* $f(\\mathbf{x})=x_{(1)}+x_{(2)}+\\dotsb+x_{(k)}$, the some of $k$ largest components of $\\mathbf{x}\\in\\mathbb{R}^d$, is convex, because\n",
    "$$\n",
    "    f(\\mathbf{x})=\\max\\{x_{i_1} +\\dotsb + x_{i_k} : 1\\le i_1 <i_2 < \\dotsb < i_k \\le n\\}.\n",
    "$$\n",
    "\n",
    "* Maximum eigenvalue of a symmetric matrix $\\lambda_{\\max}: \\mathbb{S}^d \\to \\mathbb{R}$ is convex, because\n",
    "$$\n",
    "    \\lambda_{\\max}(\\mathbf{X}) = \\max_{\\mathbf{v}^T\\mathbf{v}=1} \\mathbf{v}^T\\mathbf{X}\\mathbf{v}.\n",
    "$$\n",
    "(Rayleigh quotient). The maximand is linear (hence convex) in $\\mathbf{X}$ for each $\\mathbf{v}$.\n",
    "    - Minimum eigenvalue of a symmetric matrix is concave.\n",
    "\n",
    "* Sum of $k$ largest eigenvalues of a symmetric matrix is convex, because\n",
    "$$\n",
    "    \\sum_{i=1}^k\\lambda_{i}(\\mathbf{X}) = \\max_{\\mathbf{V}^T\\mathbf{V}=\\mathbf{I}_k, \\mathbf{V}\\in\\mathbb{R}^{d\\times k}} \\text{tr}(\\mathbf{V}^T\\mathbf{X}\\mathbf{V})\n",
    "$$\n",
    "(Ky Fan, 1949).\n",
    "    \n",
    "* Support function of a set: $\\sigma_C(\\mathbf{x}) = \\sup_{\\mathbf{y}\\in C} \\langle \\mathbf{x}, \\mathbf{y} \\rangle$ is convex (with an obvious domain), because $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$ is convex (linear) in $\\mathbf{x}$ for each $\\mathbf{y}\\in C$.\n",
    "    - Ex) dual norm: $\\|\\mathbf{x}\\|_* = \\sup_{\\|\\mathbf{y}\\| \\le 1} \\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sigma_C(\\mathbf{x})$, where $C=\\{\\mathbf{y}: \\|\\mathbf{y}\\| \\le 1\\}$ is a norm ball.\n",
    "    - Hence $\\ell_p$ norm is a support function of the unit $\\ell_q$ norm ball, $1/p+1/q=1$.\n",
    "    \n",
    "* Matrix perspective function $f(\\mathbf{x},\\mathbf{Y}) = \\mathbf{x}^T\\mathbf{Y}^{-1}\\mathbf{x}$ is convex with $\\text{dom}f=\\mathbb{R}^d\\times\\mathbb{S}_{++}^d$ because\n",
    "$$\n",
    "    \\mathbf{x}^T\\mathbf{Y}^{-1}\\mathbf{x} = \\sup_{\\mathbf{z}\\in\\mathbb{R}^d}\\{2\\mathbf{x}^T\\mathbf{z} - \\mathbf{z}^T\\mathbf{Y}\\mathbf{z}\\}.\n",
    "$$\n",
    "\n",
    "* Kullback-Leibler divergence of two $d$-dimensional probability vectors\n",
    "$$\n",
    "    D(\\mathbf{x} || \\mathbf{y}) = \\sum_{i=1}^d x_i\\log(x_i/y_i)\n",
    "$$\n",
    "(taking $0\\log 0= 0$ and $\\log 0=\\infty$ by continuity)\n",
    "is convex on $\\Delta_{d-1}\\times\\Delta_{d-1}$, because\n",
    "    1. $g(x, t)=t\\log (t/x)$ is convex since it is the perspective of $f(x)=-\\log x$;\n",
    "    2. $\\tilde{D}(\\mathbf{x}, \\mathbf{y}) = \\sum_{i=1}^d (x_i\\log(x_i/y_i) - x_i + y_i)$ is convex on $\\mathbb{R}_{+}^d\\times \\mathbb{R}_{+}^d$;\n",
    "    3. $D(\\mathbf{x} || \\mathbf{y}) = \\tilde{D}(\\mathbf{x}, \\mathbf{y}) + \\iota_{\\Delta_{d-1}}(\\mathbf{x}) + \\iota_{\\Delta_{d-1}}(\\mathbf{y})$."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
